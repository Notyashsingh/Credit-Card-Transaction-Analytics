{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "815c23f7",
   "metadata": {},
   "source": [
    "1) Master File Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bedadc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# File path\n",
    "file_path = r'D:\\Yash\\Project\\Raw File\\credit_card_transactions Master Dataset.csv'\n",
    "\n",
    "# Output folder\n",
    "output_folder = r'D:\\Yash\\Project\\Outputs'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Chunk size for reading\n",
    "chunksize = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9898e1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories CSV saved.\n"
     ]
    }
   ],
   "source": [
    "category_cols = ['category']\n",
    "categories = pd.DataFrame()\n",
    "\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "    chunk.columns = chunk.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "    chunk_cat = chunk[['category']].drop_duplicates()\n",
    "    categories = pd.concat([categories, chunk_cat], ignore_index=True)\n",
    "\n",
    "categories.drop_duplicates(subset=['category'], inplace=True)\n",
    "categories['category_id'] = range(1, len(categories)+1)\n",
    "categories.rename(columns={'category': 'category_name'}, inplace=True)\n",
    "categories = categories[['category_id', 'category_name']]\n",
    "\n",
    "# Save to CSV\n",
    "categories.to_csv(os.path.join(output_folder, 'categories.csv'), index=False)\n",
    "print(\"Categories CSV saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "698a5112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merchants CSV saved.\n"
     ]
    }
   ],
   "source": [
    "merchant_cols = ['merchant','category','merch_lat','merch_long','merch_zipcode']\n",
    "merchants = pd.DataFrame()\n",
    "\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "    chunk.columns = chunk.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "    chunk_merchants = chunk[merchant_cols].drop_duplicates(subset=['merchant'])\n",
    "    merchants = pd.concat([merchants, chunk_merchants], ignore_index=True)\n",
    "\n",
    "merchants.drop_duplicates(subset=['merchant'], inplace=True)\n",
    "\n",
    "# Map category_id from categories table\n",
    "merchants = merchants.merge(categories, left_on='category', right_on='category_name', how='left')\n",
    "\n",
    "# Assign merchant_id\n",
    "merchants['merchant_id'] = range(1, len(merchants)+1)\n",
    "merchants.rename(columns={\n",
    "    'merchant': 'merchant_name',\n",
    "    'merch_lat': 'merchant_lat',\n",
    "    'merch_long': 'merchant_long',\n",
    "    'merch_zipcode': 'merchant_zipcode'\n",
    "}, inplace=True)\n",
    "\n",
    "merchants = merchants[['merchant_id','merchant_name','category_id','merchant_lat','merchant_long','merchant_zipcode']]\n",
    "\n",
    "# Save to CSV\n",
    "merchants.to_csv(os.path.join(output_folder, 'merchants.csv'), index=False)\n",
    "print(\"Merchants CSV saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dfde509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date table CSV saved.\n"
     ]
    }
   ],
   "source": [
    "date_table = pd.DataFrame()\n",
    "\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "    chunk.columns = chunk.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "    chunk['trans_date'] = pd.to_datetime(chunk['trans_date_trans_time'].str.split(' ').str[0], errors='coerce')\n",
    "    date_table = pd.concat([date_table, chunk[['trans_date']]], ignore_index=True)\n",
    "\n",
    "date_table.drop_duplicates(subset=['trans_date'], inplace=True)\n",
    "date_table['date_id'] = date_table['trans_date']\n",
    "date_table['year'] = date_table['trans_date'].dt.year\n",
    "date_table['quarter'] = 'Q' + date_table['trans_date'].dt.quarter.astype(str)\n",
    "date_table['month_number'] = date_table['trans_date'].dt.month\n",
    "date_table['month_name'] = date_table['trans_date'].dt.month_name()\n",
    "date_table['week_number'] = date_table['trans_date'].dt.isocalendar().week\n",
    "date_table['day_of_week'] = date_table['trans_date'].dt.day_name()\n",
    "date_table['is_weekend'] = date_table['day_of_week'].isin(['Saturday','Sunday'])\n",
    "\n",
    "date_table = date_table[['date_id','year','quarter','month_number','month_name','week_number','day_of_week','is_weekend']]\n",
    "\n",
    "# Save to CSV\n",
    "date_table.to_csv(os.path.join(output_folder, 'date_table.csv'), index=False)\n",
    "print(\"Date table CSV saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0544fd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Freshcore2025\\AppData\\Local\\Temp\\ipykernel_18724\\3651082439.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  unique_customers = pd.concat([unique_customers, chunk_customers], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer helper table saved: customer_map.csv\n",
      "Customers dimension table saved: customers.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# File path and output folder\n",
    "file_path = r'D:\\Yash\\Project\\Raw File\\credit_card_transactions Master Dataset.csv'\n",
    "output_folder = r'D:\\Yash\\Project\\Outputs'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "chunksize = 100000\n",
    "\n",
    "def normalize_columns(df):\n",
    "    return df.rename(columns=lambda x: x.strip().lower().replace(\" \", \"_\"))\n",
    "\n",
    "# Columns to extract for customers\n",
    "customer_cols = ['cc_num','first','last','gender','street','city','state','zip','lat','long','city_pop','job','dob']\n",
    "unique_customers = pd.DataFrame(columns=customer_cols)\n",
    "\n",
    "# Read in chunks and accumulate unique customers\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "    chunk = normalize_columns(chunk)\n",
    "    chunk_customers = chunk[customer_cols].drop_duplicates(subset=['cc_num'])\n",
    "    unique_customers = pd.concat([unique_customers, chunk_customers], ignore_index=True)\n",
    "\n",
    "# Deduplicate across all chunks\n",
    "unique_customers.drop_duplicates(subset=['cc_num'], inplace=True)\n",
    "\n",
    "# Assign numeric customer_id\n",
    "unique_customers['customer_id'] = range(1, len(unique_customers)+1)\n",
    "\n",
    "# Save helper mapping table (customer_id â†” cc_num)\n",
    "customer_map = unique_customers[['customer_id','cc_num']]\n",
    "customer_map.to_csv(os.path.join(output_folder, 'customer_map.csv'), index=False)\n",
    "print(\"Customer helper table saved: customer_map.csv\")\n",
    "\n",
    "# Generate final customers dimension table\n",
    "unique_customers['full_name'] = unique_customers['first'] + ' ' + unique_customers['last']\n",
    "unique_customers['dob'] = pd.to_datetime(unique_customers['dob'], errors='coerce')\n",
    "unique_customers['age'] = (pd.Timestamp('today') - unique_customers['dob']).dt.days // 365\n",
    "\n",
    "unique_customers.rename(columns={\n",
    "    'zip': 'zipcode',\n",
    "    'lat': 'latitude',\n",
    "    'long': 'longitude',\n",
    "    'city_pop': 'city_population'\n",
    "}, inplace=True)\n",
    "\n",
    "customers = unique_customers[['customer_id','full_name','first','last','gender',\n",
    "                              'street','city','state','zipcode','latitude',\n",
    "                              'longitude','city_population','job','dob','age']]\n",
    "customers.to_csv(os.path.join(output_folder, 'customers.csv'), index=False)\n",
    "print(\"Customers dimension table saved: customers.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "300245ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 processed\n",
      "Chunk 2 processed\n",
      "Chunk 3 processed\n",
      "Chunk 4 processed\n",
      "Chunk 5 processed\n",
      "Chunk 6 processed\n",
      "Chunk 7 processed\n",
      "Chunk 8 processed\n",
      "Chunk 9 processed\n",
      "Chunk 10 processed\n",
      "Chunk 11 processed\n",
      "Chunk 12 processed\n",
      "Chunk 13 processed\n",
      "Transactions fact table saved successfully with numeric transaction_id.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "output_folder = r'D:\\Yash\\Project\\Outputs'\n",
    "file_path = r'D:\\Yash\\Project\\Raw File\\credit_card_transactions Master Dataset.csv'\n",
    "transactions_output = os.path.join(output_folder, 'transactions.csv')\n",
    "chunksize = 100000\n",
    "\n",
    "# Load helper/reference tables\n",
    "customer_map = pd.read_csv(os.path.join(output_folder, 'customer_map.csv'))  # columns: cc_num, customer_id\n",
    "merchants = pd.read_csv(os.path.join(output_folder, 'merchants.csv'))      # columns: merchant_id, merchant_name\n",
    "categories = pd.read_csv(os.path.join(output_folder, 'categories.csv'))    # columns: category_id, category_name\n",
    "\n",
    "# Normalize reference tables for safe merge\n",
    "merchants['merchant_name'] = merchants['merchant_name'].astype(str).str.strip().str.lower()\n",
    "categories['category_name'] = categories['category_name'].astype(str).str.strip().str.lower()\n",
    "\n",
    "transaction_cols = ['trans_num','trans_date_trans_time','cc_num','merchant','category','amt','is_fraud']\n",
    "\n",
    "is_first_write = True\n",
    "transaction_id_counter = 1  # numeric transaction ID\n",
    "\n",
    "def normalize_columns(df):\n",
    "    return df.rename(columns=lambda x: x.strip().lower().replace(\" \", \"_\"))\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunksize)):\n",
    "    chunk = normalize_columns(chunk)\n",
    "    tx = chunk[transaction_cols].copy()\n",
    "\n",
    "    # Map customer_id using helper table\n",
    "    tx = tx.merge(customer_map, on='cc_num', how='left')\n",
    "    tx.drop(columns=['cc_num'], inplace=True)\n",
    "\n",
    "    # Normalize text columns in master file\n",
    "    tx['merchant'] = tx['merchant'].astype(str).str.strip().str.lower()\n",
    "    tx['category'] = tx['category'].astype(str).str.strip().str.lower()\n",
    "\n",
    "    # Merge merchant_id\n",
    "    tx = tx.merge(merchants[['merchant_id','merchant_name']], \n",
    "                  left_on='merchant', right_on='merchant_name', how='left')\n",
    "    tx.drop(columns=['merchant','merchant_name'], inplace=True)\n",
    "\n",
    "    # Merge category_id\n",
    "    tx = tx.merge(categories[['category_id','category_name']], \n",
    "                  left_on='category', right_on='category_name', how='left')\n",
    "    tx.drop(columns=['category','category_name'], inplace=True)\n",
    "\n",
    "    # Check for missing mappings\n",
    "    missing_customer = tx['customer_id'].isnull().sum()\n",
    "    missing_merchant = tx['merchant_id'].isnull().sum()\n",
    "    missing_category = tx['category_id'].isnull().sum()\n",
    "    if missing_customer > 0 or missing_merchant > 0 or missing_category > 0:\n",
    "        print(f\"Chunk {i+1} warnings - missing IDs: \"\n",
    "              f\"customer={missing_customer}, merchant={missing_merchant}, category={missing_category}\")\n",
    "\n",
    "    # Split transaction datetime\n",
    "    tx['trans_date_trans_time'] = pd.to_datetime(tx['trans_date_trans_time'], errors='coerce')\n",
    "    tx['transaction_date'] = tx['trans_date_trans_time'].dt.date\n",
    "    tx['transaction_time'] = tx['trans_date_trans_time'].dt.time\n",
    "    tx.drop(columns=['trans_date_trans_time'], inplace=True)\n",
    "\n",
    "    # Rename amount\n",
    "    tx.rename(columns={'amt':'amount'}, inplace=True)\n",
    "\n",
    "    # Assign numeric unique transaction_id\n",
    "    tx['transaction_id'] = range(transaction_id_counter, transaction_id_counter + len(tx))\n",
    "    transaction_id_counter += len(tx)\n",
    "\n",
    "    # Reorder final columns\n",
    "    final_cols = ['transaction_id','customer_id','merchant_id','category_id',\n",
    "                  'transaction_date','transaction_time','amount','is_fraud']\n",
    "    tx = tx[final_cols]\n",
    "\n",
    "    # Append to CSV\n",
    "    tx.to_csv(transactions_output, index=False, mode='w' if is_first_write else 'a', header=is_first_write)\n",
    "    is_first_write = False\n",
    "    print(f\"Chunk {i+1} processed\")\n",
    "\n",
    "print(\"Transactions fact table saved successfully with numeric transaction_id.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963fb002",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import pandas as pd\n",
    "import os\n",
    "\n",
    "# File path and output folder\n",
    "file_path = r'D:\\Yash\\Project\\Raw File\\credit_card_transactions Master Dataset.csv'\n",
    "output_folder = r'D:\\Yash\\Project\\Outputs'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "chunksize = 100000\n",
    "\n",
    "def normalize_columns(df):\n",
    "    return df.rename(columns=lambda x: x.strip().lower().replace(\" \", \"_\"))\n",
    "\n",
    "# --------------------\n",
    "# Step 1: Create customer helper table (cc_num -> customer_id)\n",
    "# --------------------\n",
    "customer_cols = ['cc_num','first','last','gender','street','city','state','zip','lat','long','city_pop','job','dob']\n",
    "unique_customers = pd.DataFrame(columns=customer_cols)\n",
    "\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "    chunk = normalize_columns(chunk)\n",
    "    chunk_customers = chunk[customer_cols].drop_duplicates(subset=['cc_num'])\n",
    "    unique_customers = pd.concat([unique_customers, chunk_customers], ignore_index=True)\n",
    "\n",
    "unique_customers.drop_duplicates(subset=['cc_num'], inplace=True)\n",
    "unique_customers['customer_id'] = range(1, len(unique_customers)+1)\n",
    "\n",
    "# Save helper mapping table\n",
    "customer_map = unique_customers[['customer_id','cc_num']]\n",
    "customer_map.to_csv(os.path.join(output_folder, 'customer_map.csv'), index=False)\n",
    "print(\"Customer helper table saved: customer_map.csv\")\n",
    "\n",
    "# --------------------\n",
    "# Step 2: Create final customers.csv (dimension table)\n",
    "# --------------------\n",
    "unique_customers['full_name'] = unique_customers['first'] + ' ' + unique_customers['last']\n",
    "unique_customers['dob'] = pd.to_datetime(unique_customers['dob'], errors='coerce')\n",
    "unique_customers['age'] = (pd.Timestamp('today') - unique_customers['dob']).dt.days // 365\n",
    "\n",
    "unique_customers.rename(columns={\n",
    "    'zip': 'zipcode',\n",
    "    'lat': 'latitude',\n",
    "    'long': 'longitude',\n",
    "    'city_pop': 'city_population'\n",
    "}, inplace=True)\n",
    "\n",
    "customers = unique_customers[['customer_id','full_name','first','last','gender',\n",
    "                              'street','city','state','zipcode','latitude',\n",
    "                              'longitude','city_population','job','dob','age']]\n",
    "customers.to_csv(os.path.join(output_folder, 'customers.csv'), index=False)\n",
    "print(\"Customers dimension table saved: customers.csv\")\n",
    "\n",
    "# --------------------\n",
    "# Step 3: Create transactions.csv (fact table) with numeric transaction_id\n",
    "# --------------------\n",
    "transaction_cols = [\n",
    "    'trans_num','trans_date_trans_time','cc_num','merchant','category',\n",
    "    'amt','unix_time','merch_lat','merch_long','is_fraud'\n",
    "]\n",
    "\n",
    "transactions_output = os.path.join(output_folder, 'transactions_old.csv')\n",
    "is_first_write = True\n",
    "transaction_id_counter = 1  # numeric transaction ID starting from 1\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunksize)):\n",
    "    chunk = normalize_columns(chunk)\n",
    "    tx = chunk[transaction_cols].copy()\n",
    "\n",
    "    # Map customer_id using helper table\n",
    "    tx = tx.merge(customer_map, on='cc_num', how='left')\n",
    "\n",
    "    # Drop cc_num for privacy\n",
    "    tx.drop(columns=['cc_num'], inplace=True)\n",
    "\n",
    "    # Rename columns\n",
    "    tx.rename(columns={\n",
    "        'trans_num': 'old_transaction_id',  # optional reference\n",
    "        'trans_date_trans_time': 'transaction_date',\n",
    "        'amt': 'amount'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Assign unique numeric transaction_id\n",
    "    tx['transaction_id'] = range(transaction_id_counter, transaction_id_counter + len(tx))\n",
    "    transaction_id_counter += len(tx)\n",
    "\n",
    "    # Reorder columns\n",
    "    tx = tx[['transaction_id','customer_id','merchant','category','transaction_date',\n",
    "             'unix_time','merch_lat','merch_long','amount','is_fraud']]\n",
    "\n",
    "    # Append to CSV\n",
    "    tx.to_csv(transactions_output, index=False, mode='w' if is_first_write else 'a', header=is_first_write)\n",
    "    is_first_write = False\n",
    "\n",
    "    print(f\"Chunk {i+1} processed\")\n",
    "\n",
    "print(\"Transactions fact table saved: transactions.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad3b73b",
   "metadata": {},
   "source": [
    "2) Connct with Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be013196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "try:\n",
    "    # Attempt to connect\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",        # or your server\n",
    "        dbname=\"cc_analysis\",     # your database name\n",
    "        user=\"postgres\",        # your username\n",
    "        password=\"YashSingh@1205\" # your password\n",
    "    )\n",
    "    print(\"Connection successful!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Connection failed:\", e)\n",
    "\n",
    "finally:\n",
    "    # Close connection if it was opened\n",
    "    if 'conn' in locals() and conn:\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436500f2",
   "metadata": {},
   "source": [
    "Importing data into categories table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c862acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories uploaded successfully: 14 rows.\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Database connection\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    dbname=\"cc_analysis\",\n",
    "    user=\"postgres\",\n",
    "    password=\"YashSingh@1205\",\n",
    "    port=5432\n",
    ")\n",
    "\n",
    "categories_file = r\"D:\\Yash\\Project\\Outputs\\categories.csv\"\n",
    "\n",
    "try:\n",
    "    with conn:\n",
    "        with conn.cursor() as cur:\n",
    "            # Load categories CSV into table\n",
    "            with open(categories_file, 'r', encoding='utf-8') as f:\n",
    "                cur.copy_expert(\"COPY categories FROM STDIN WITH CSV HEADER\", f)\n",
    "            \n",
    "            cur.execute(\"SELECT COUNT(*) FROM categories;\")\n",
    "            count = cur.fetchone()[0]\n",
    "            print(f\"Categories uploaded successfully: {count} rows.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Upload failed:\", e)\n",
    "finally:\n",
    "    conn.close()\n",
    "    print(\"Database connection closed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862410ef",
   "metadata": {},
   "source": [
    "Importing data into customers table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa05f913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers uploaded successfully: 983 rows.\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Database connection\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    dbname=\"cc_analysis\",\n",
    "    user=\"postgres\",\n",
    "    password=\"YashSingh@1205\",\n",
    "    port=5432\n",
    ")\n",
    "\n",
    "customers_file = r\"D:\\Yash\\Project\\Outputs\\customers.csv\"\n",
    "\n",
    "try:\n",
    "    with conn:\n",
    "        with conn.cursor() as cur:\n",
    "            # Load customers CSV into table\n",
    "            with open(customers_file, 'r', encoding='utf-8') as f:\n",
    "                cur.copy_expert(\"COPY customers FROM STDIN WITH CSV HEADER\", f)\n",
    "            \n",
    "            cur.execute(\"SELECT COUNT(*) FROM customers;\")\n",
    "            count = cur.fetchone()[0]\n",
    "            print(f\"Customers uploaded successfully: {count} rows.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Upload failed:\", e)\n",
    "finally:\n",
    "    conn.close()\n",
    "    print(\"Database connection closed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7bb18f",
   "metadata": {},
   "source": [
    "Importing data into Merchants table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09a9045b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merchants uploaded successfully, 693 rows.\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Database connection\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    dbname=\"cc_analysis\",\n",
    "    user=\"postgres\",\n",
    "    password=\"YashSingh@1205\",\n",
    "    port=5432\n",
    ")\n",
    "\n",
    "merchant_csv = r\"D:\\Yash\\Project\\Outputs\\merchants.csv\"\n",
    "\n",
    "try:\n",
    "    with conn:\n",
    "        with conn.cursor() as cur:\n",
    "            # Optional: Clear table first\n",
    "            cur.execute(\"TRUNCATE TABLE merchants;\")\n",
    "            \n",
    "            # Load CSV into table\n",
    "            with open(merchant_csv, 'r', encoding='utf-8') as f:\n",
    "                cur.copy_expert(\"COPY merchants FROM STDIN WITH CSV HEADER\", f)\n",
    "            \n",
    "            # Row count check\n",
    "            cur.execute(\"SELECT COUNT(*) FROM merchants;\")\n",
    "            print(f\"Merchants uploaded successfully, {cur.fetchone()[0]} rows.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Upload failed:\", e)\n",
    "finally:\n",
    "    conn.close()\n",
    "    print(\"Database connection closed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56486ad0",
   "metadata": {},
   "source": [
    "Importing data into Date table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de235127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date table uploaded successfully, 537 rows.\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Database connection\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    dbname=\"cc_analysis\",\n",
    "    user=\"postgres\",\n",
    "    password=\"YashSingh@1205\",\n",
    "    port=5432\n",
    ")\n",
    "\n",
    "date_csv = r\"D:\\Yash\\Project\\Outputs\\date_table.csv\"\n",
    "\n",
    "try:\n",
    "    with conn:\n",
    "        with conn.cursor() as cur:\n",
    "            # Optional: clear table first\n",
    "            cur.execute(\"TRUNCATE TABLE date_table;\")\n",
    "            \n",
    "            # Load CSV into table\n",
    "            with open(date_csv, 'r', encoding='utf-8') as f:\n",
    "                cur.copy_expert(\"COPY date_table FROM STDIN WITH CSV HEADER\", f)\n",
    "            \n",
    "            # Row count check\n",
    "            cur.execute(\"SELECT COUNT(*) FROM date_table;\")\n",
    "            print(f\"Date table uploaded successfully, {cur.fetchone()[0]} rows.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Upload failed:\", e)\n",
    "finally:\n",
    "    conn.close()\n",
    "    print(\"Database connection closed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3210f4e2",
   "metadata": {},
   "source": [
    "Importing data into transactions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09051412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions table uploaded successfully, 1296675 rows.\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Database connection\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    dbname=\"cc_analysis\",\n",
    "    user=\"postgres\",\n",
    "    password=\"YashSingh@1205\",\n",
    "    port=5432\n",
    ")\n",
    "\n",
    "transactions_csv = r\"D:\\Yash\\Project\\Outputs\\transactions.csv\"\n",
    "\n",
    "try:\n",
    "    with conn:\n",
    "        with conn.cursor() as cur:\n",
    "            # Optional: clear table first\n",
    "            cur.execute(\"TRUNCATE TABLE transactions;\")\n",
    "            \n",
    "            # Load CSV into table\n",
    "            with open(transactions_csv, 'r', encoding='utf-8') as f:\n",
    "                cur.copy_expert(\"COPY transactions FROM STDIN WITH CSV HEADER\", f)\n",
    "            \n",
    "            # Row count check\n",
    "            cur.execute(\"SELECT COUNT(*) FROM transactions;\")\n",
    "            print(f\"Transactions table uploaded successfully, {cur.fetchone()[0]} rows.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Upload failed:\", e)\n",
    "finally:\n",
    "    conn.close()\n",
    "    print(\"Database connection closed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
